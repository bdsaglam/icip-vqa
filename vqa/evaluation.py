# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/07_evaluation.ipynb (unless otherwise specified).

__all__ = ['evaluate_mtl']

# Cell

import matplotlib.pyplot as plt
from fastai.basics import *

# Cell

import warnings
from sklearn.exceptions import UndefinedMetricWarning
from sklearn.metrics import classification_report, ConfusionMatrixDisplay
from fastai.metrics import F1Score, accuracy

warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning, module=r'.*')

def evaluate_mtl(vocabs, targets, preds, show=False):
    for vocab, target, pred in zip(vocabs, targets, preds):
        label_indices = list(range(len(vocab)))
        y_true, y_pred = target.cpu().numpy(), pred.cpu().numpy()
        clf_report = classification_report(y_true, y_pred, labels=label_indices, target_names=vocab)
        if show:
            print(clf_report)
            fig, ax = plt.subplots(figsize=(16, 12))
            ConfusionMatrixDisplay.from_predictions(y_true, y_pred, labels=label_indices, display_labels=vocab, ax=ax)

    distortion_f1_macro = F1Score(average='macro')(preds[0], targets[0])
    distortion_accuracy = accuracy(probs[0], targets[0])
    severity_accuracy = accuracy(probs[1], targets[1])
    return dict(
        classification_report=clf_report,
        distortion_f1_macro=distortion_f1_macro,
        distortion_accuracy=distortion_accuracy,
        severity_accuracy=severity_accuracy,
    )